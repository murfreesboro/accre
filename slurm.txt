
#########################
scontrol:
#########################

scontrol show -dd job=jobID
this is used to show the "running job" status. -dd will print out the slurm script for that job.

#########################
squeue - Job States
#########################
R - Job is running on compute nodes
PD - Job is waiting on compute nodes
CG - Job is completing

#########################
# slurm database and log
#########################
for the C7, you do "ssh sched-vip"
this server will let you enter into the slurm server box. And check the /var/log/slurm/slurmctld*.log
this is the slurm log for the slurm jobs

for the C6, "ssh slurmsched1"; then check the /var/log/messages

#########################
# slurm can not produce an output file:
#########################
Tipically when jobs die like that it is for two reasons that I have encountered:
1. The shebang has some typos and the script cannot be interpreted
2. Slurm cannot open the output file (wrong file path, no write permissions in the submission directory, quota limit reached, etc.)
these suggestions are from Davide.


#########################
# options with sbatch
#########################
the line #SBATCH --constraint=sandybridge,haswell,skylake works for the slurm, it means the compute nodes could be either of the
choice listed. The line #SBATCH --constraint=[sandybridge,haswell,skylake] means the job will pick up one type of compute node 
for the job,  so all the allocated nodes belonging to the same architecture. We can also replace the "," as "|", like 
#SBATCH --constraint=[sandybridge|haswell|skylake].





